{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "wgan_gp_code",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "zUMEjX2BqV8n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python2\n",
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Sat Mar 23 13:20:15 2019\n",
        "\n",
        "@author: chin-weihuang\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F\n",
        "import samplers\n",
        "\n",
        "\n",
        "############### import the sampler ``samplers.distribution4'' \n",
        "############### train a discriminator on distribution4 and standard gaussian\n",
        "############### estimate the density of distribution4\n",
        "\n",
        "#######--- INSERT YOUR CODE BELOW ---#######\n",
        "      \n",
        "class Critic(nn.Module):\n",
        "    \n",
        "    def __init__(self,input_size=1,hidden_size=10,output_size=1):\n",
        "        super(Critic, self).__init__()\n",
        "        \n",
        "        self.minibatch_size = 512\n",
        "        self.lr = 1e-3\n",
        "        self.epoch_count = 2\n",
        "        \n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(input_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, output_size),\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "          \n",
        "        return self.layers(x)\n",
        "      \n",
        "def WGAN_GPloss(x_val, minibatch_size, epoch_count, learning_rate, losses,\\\n",
        "                input_size, hidden_size, output_size, penalty_coef = 10,\\\n",
        "                real_sampler=samplers.distribution1,\\\n",
        "                fake_sampler=samplers.distribution1,\\\n",
        "               ):\n",
        "  #critic.zero_grad()\n",
        "  critic = Critic(input_size, hidden_size, output_size).cuda()\n",
        "  optimizer = optim.SGD(critic.parameters(),lr = learning_rate)\n",
        "\n",
        "  #get data for both distributions from samplers\n",
        "  real_dist = iter(real_sampler(x_val,512))\n",
        "  real_samples = next(real_dist)\n",
        "  real_tensor_samples = torch.tensor(real_samples, requires_grad = True).float().cuda()\n",
        "\n",
        "  fake_dist = iter(fake_sampler(0,512)) \n",
        "  fake_samples = next(fake_dist)\n",
        "  fake_tensor_samples = torch.tensor(fake_samples, requires_grad = True).float().cuda()\n",
        "\n",
        "################################gradient penalty################################\n",
        "  \n",
        "################################################################################\n",
        "  \n",
        "  for i in range(epoch_count):\n",
        "    critic.zero_grad()\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    real_critic_output = critic(real_tensor_samples)\n",
        "    fake_critic_output = critic(fake_tensor_samples)\n",
        "    \n",
        "    #make mix\n",
        "    alpha = torch.rand([1]).cuda() #mixing coefficient, might need to be made 512 dim\n",
        "    z = alpha*real_tensor_samples + (1-alpha)*fake_tensor_samples\n",
        "    critic_z = critic(z)\n",
        "\n",
        "    #compute gradient of critic(z) w.r.t. z\n",
        "    grads = torch.autograd.grad(outputs = critic_z, \\\n",
        "                                inputs = z, \\\n",
        "                                grad_outputs = torch.ones([512,2]).cuda(), \\\n",
        "                                #might have to change dimension for grad_outputs\n",
        "                                only_inputs = True, \\\n",
        "                                create_graph = True,\n",
        "                                retain_graph = True\n",
        "                               )[0]\n",
        "    \n",
        "    #compute the full penalty (coef*mean of norm of grad)\n",
        "    grad_penalty = ((torch.norm(grads, p=2, dim=1)-1)**2)\n",
        "  \n",
        "    real_critic_output_expected = real_critic_output.mean()\n",
        "    fake_critic_output_expected = fake_critic_output.mean()\n",
        "    grad_penalty_mean = grad_penalty.mean()\n",
        "    \n",
        "    total_output = -real_critic_output_expected + fake_critic_output_expected + \\\n",
        "                    penalty_coef*(grad_penalty_mean)\n",
        "    \n",
        "    total_output.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "  return -total_output\n",
        "#   return total_output\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wwt44dVs54R0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "distances = []\n",
        "for phi in np.linspace(-1,1,21):\n",
        "  wgan_gp = WGAN_GPloss(x_val=phi, minibatch_size=512, epoch_count=1000, learning_rate = 1e-3, losses = losses,\\\n",
        "                        input_size=2, hidden_size=10, output_size=1, \\\n",
        "                        real_sampler=samplers.distribution1,\\\n",
        "                        fake_sampler=samplers.distribution1,\\\n",
        "                       )\n",
        "  distances.append(wgan_gp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pz9rgHYxBBMT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "wgan_gp = WGAN_GPloss(x_val=0, minibatch_size=512, epoch_count=100, learning_rate = 1e-3, losses = losses,\\\n",
        "                      input_size=2, hidden_size=10, output_size=1, \\\n",
        "                      real_sampler=samplers.distribution1,\\\n",
        "                      fake_sampler=samplers.distribution1,\\\n",
        "                     )\n",
        "\n",
        "# losses_investigation\n",
        "losses = []\n",
        "# for i in np.linspace(-1,1,21):\n",
        "critic = Critic(input_size=2, hidden_size=10, output_size=1).cuda()\n",
        "optimizer = optim.SGD(critic.parameters(),lr = 1e-3)\n",
        "\n",
        "#get data for both distributions from samplers\n",
        "real_dist = iter(samplers.distribution1(1,512))\n",
        "real_samples = next(real_dist)\n",
        "real_tensor_samples = torch.tensor(real_samples, requires_grad = True).float().cuda()\n",
        "\n",
        "fake_dist = iter(samplers.distribution1(0,512)) \n",
        "fake_samples = next(fake_dist)\n",
        "fake_tensor_samples = torch.tensor(fake_samples, requires_grad = True).float().cuda()\n",
        "penalty_coef = 10 #or higher if needed\n",
        "\n",
        "\n",
        "for j in range(10000):\n",
        "  critic.zero_grad()\n",
        "  optimizer.zero_grad()\n",
        "  real_critic_output = critic(real_tensor_samples)\n",
        "  fake_critic_output = critic(fake_tensor_samples)\n",
        "\n",
        "  alpha = torch.rand([1]).cuda() #mixing coefficient\n",
        "  z = alpha*real_tensor_samples + (1-alpha)*fake_tensor_samples\n",
        "  critic_z = critic(z)\n",
        "\n",
        "  grads = torch.autograd.grad(outputs = critic_z, \\\n",
        "                              inputs = z, \\\n",
        "                              grad_outputs = torch.ones([512,2]).cuda(), \\\n",
        "                              #might have to change dimension for grad_outputs\n",
        "                              only_inputs = True, \\\n",
        "                              create_graph = True,\n",
        "                              retain_graph = True\n",
        "                             )[0]\n",
        "  #compute the full penalty (coef*mean of norm of grad)\n",
        "  grad_penalty = ((torch.norm(grads, p=2, dim=1)-1)**2)\n",
        "\n",
        "  real_critic_output_expected = real_critic_output.mean()\n",
        "  fake_critic_output_expected = fake_critic_output.mean()\n",
        "  grad_penalty_mean = grad_penalty.mean()\n",
        "\n",
        "  total_output = -real_critic_output_expected + fake_critic_output_expected + \\\n",
        "                  penalty_coef*(grad_penalty_mean)\n",
        "\n",
        "  total_output.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  losses.append(-total_output)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6_FW7Uz-W-nt",
        "colab_type": "code",
        "outputId": "82078f69-3c65-4584-d839-1d6f51b7388a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2046
        }
      },
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "losses[-100:]"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor(0.5051, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5069, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5049, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5050, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5050, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5061, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5061, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5069, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5049, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5050, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5052, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5053, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5060, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5050, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5061, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5060, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5052, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5057, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5058, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5055, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5066, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5056, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5059, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5066, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5064, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5056, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5059, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5066, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5062, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5056, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5052, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5062, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5062, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5063, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5063, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5063, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5056, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5063, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5066, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5064, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5068, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5063, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5066, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5061, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5053, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5067, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5058, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5065, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5054, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5057, device='cuda:0', grad_fn=<NegBackward>),\n",
              " tensor(0.5058, device='cuda:0', grad_fn=<NegBackward>)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "Tnb82UnC0rp4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}